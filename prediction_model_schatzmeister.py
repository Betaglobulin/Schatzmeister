# -*- coding: utf-8 -*-
"""prediction-model_schatzmeister.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qpPS61vZLwjt4JBXcwUX1NBrq6pjgwa8
"""

#import packages
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
import warnings
warnings.filterwarnings('ignore')
import pandas.util.testing as tm

#Load data 
def load_data():
    ts = pd.read_csv('ergebnis_28082020_loesch_inkl_duplikate.csv', sep = ";")
    zeit = [ts.loc[i, 'zeit'].split() for i in range(len(ts))] #time stamp aufsplitten
    datum = [zeit[i][0] for i in range(len(zeit))] #rechten time stamp 
    # datum ist eine Liste
    ts['datum'] = datum
    ts['datum'] = pd.to_datetime(ts['datum'], format='%d.%m.%Y') #konvertieren zu Typ "Datum"
    ts = ts[ts['datum'] != ts['datum'].min()] #23.07 rausnehmen, da zu dem Tag sehr wenig Daten vorhanden waren
    ts['anzahl'] = 1
    
    return ts

#Pivot data 
def pivot_data(ts):
    ts_pivot = pd.pivot_table(ts, index='datum', values=['anzahl'], aggfunc='sum')
    
    return ts_pivot

#Use log to transform data
def fix_skewed_data(ts, col): #Links- und / oder Rechtsverschiebung wird ausgeglichen,
# da man eine Normalverteilung anstrebt
    ts[col] = np.log(ts[col]) 

    return ts

#Scale target using the standard scaler
#hier erfolgt noch keine Ausfsplittung in Test- und Trainingsdatensatz
#target ist das, was ich vorhersagen will
def scale_targets(y_train, y_test, col):
    sc = StandardScaler()
    y_train['target'] = sc.fit_transform(y_train[[col]])
    y_test['target'] = sc.transform(y_test[[col]])
    
    return sc, y_train, y_test

#Scale list of features using the standard scaler
#um features zu skalieren
def scale_features(X_train, X_test, col_list):
    for col in col_list:
        sc = StandardScaler()
        X_train[col] = sc.fit_transform(X_train[[col]])
        X_test[col] = sc.transform(X_test[[col]])
        
    return X_train, X_test

#Get the year, week and weekday information from the datetime index
def add_isodate(ts):
    ts[['year', 'week', 'weekday']] = pd.DataFrame( #füge drei Spalten hinzu
        ts.index.map(lambda x: x.isocalendar()).tolist(), index=ts.index)
    
    return ts

#Get the month information from the datetime index
#Monat hinzufügen
def add_month(ts):
    ts['month'] = ts.index.month
    
    return ts

#Calculate the mape / calculate error
def calculate_mape(y, y_hat):
    perc_err = (100 * (y - y_hat)) / y #Abweichung, inwiweit die Vorhersage von den Testdaten abweicht in Prozent
    # y und y_hat sind Listen
    
    return pd.np.mean(abs(perc_err)) #Durchschnitt von Betrag nutzen (abs = absolut)

#Create two plots showing the target, prediction and error
def plot_error(ts, target):
    ts['error'] = ts['prediction'] - ts[target]
    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18, 12))
    ts[[target, 'prediction']].plot(ax=axes[0])
    ts['error'].plot(ax=axes[1])

from sklearn.tree import DecisionTreeRegressor

# sämtliche Funktionen werden aufgerufen
df = load_data()
df = pivot_data(df)
df = add_isodate(df)
df = add_month(df)
df = fix_skewed_data(df, 'anzahl')

X = df.loc[:, ['year', 'month', 'week', 'weekday']] # feature df #alle Zeilen 
#--> deshalb Doppelpunkt, aber nur die aufgeführten Spalten
y = df.loc[:, ['anzahl']] # target df (Vorhersage)

end = X.index.max() #setze ein Enddatum, nämlich das höchste Datum im df
start = end - pd.Timedelta(10, unit='D') #d = days #10 Tage genommen für den Testdatensatz
# ggf. noch anpassen, da man ungefähr 1/5 an Testdatensatz haben soll
# 1/5 Länge von dem * 0,2 geht vielleicht
X_train = X[(X.index < start) | (X.index > end)] # alle, die in X sind, 
# wo das Datum kleiner als Start oder größer als Enddatum
X_test = X[(X.index >= start) & (X.index <= end)]
y_train = y[(y.index < start) | (y.index > end)]
y_test = y[(y.index >= start) & (y.index <= end)]

sc, y_train, y_test = scale_targets(y_train, y_test, 'anzahl') #benutze die Funktion scale_targets
# oben wurde die Definition geschrieben mit dem Element "col"
# Funktion scale_targets weiß, dadurch, dass es das dritte Element in der Klammer ist,
# dass col nur ein Platzhalter ist und jetzt "anzahl" gewählt wird
# --> Funktion ist damit generischer geschrieben
reg = DecisionTreeRegressor()
reg.fit(X_train, y_train.loc[:, y_train.columns == 'target']) # target ist das skalierte "Anzahl",
# deshalb wollen wir hier jetzt nicht mehr Anzahl vorhersagen, sondern das 
# Skalierte davon
# alle Zeilen in y_train, wo column = target ist

y_test['prediction_sc'] = reg.predict(X_test) #neue Spalte erstellen
# Vorhersage ist noch skaliert
y_test['prediction'] = sc.inverse_transform(y_test[['prediction_sc']])
# Skalierung wird zurückgesetzt zu einer nicht mehr skalierten Vorhersage

y_test['anzahl'] = np.exp(y_test.anzahl) #np_log auch hier rückgängigmachen
# log haben wir oben verwendet bei fix-skewed
y_test['prediction'] = np.exp(y_test.prediction) # prediction basierte auch noch auf
# Logarithmus, das muss hier jetzt auch rückgängig gemacht werden

mape = calculate_mape(y_test.anzahl, y_test.prediction)
print(mape)
# Abweichung in Prozent der Vorhersage zur Originaldata

plot_error(y_test, 'anzahl')